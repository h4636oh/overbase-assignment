{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration\n",
    "import pandas as pd\n",
    "from ollama import Client\n",
    "from ddgs import DDGS\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"gemini-3-flash-preview:cloud\"\n",
    "INPUT_FILE = \"overbase_list.csv\"\n",
    "MISSING_DATA_FILE = \"missing_data_rows.csv\"\n",
    "OUTPUT_FILE = \"processed_data.csv\"\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "print(f\"Ollama initialized with Cloud Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions (Search, LLM, Regex)\n",
    "\n",
    "COMPANY_DOMAIN_CACHE = {}\n",
    "\n",
    "def search_web(query, max_results=3):\n",
    "    \"\"\"Searches DuckDuckGo and returns a summary string.\"\"\"\n",
    "    try:\n",
    "        time.sleep(1) \n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        if not results: return \"\"\n",
    "        return \"\\n\".join([f\"- {r['body']}\" for r in results])\n",
    "    except Exception as e:\n",
    "        print(f\"   [Search Error] {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def ask_ollama(prompt, context=\"\"):\n",
    "    \"\"\"Queries the Ollama Cloud model.\"\"\"\n",
    "    full_prompt = f\"\"\"\n",
    "    Context Data:\n",
    "    {context}\n",
    "    \n",
    "    Instruction:\n",
    "    {prompt}\n",
    "    \n",
    "    Output Rules:\n",
    "    - Return ONLY the requested answer.\n",
    "    - No markdown formatting (like **bold**), no filler text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat(model=MODEL_NAME, messages=[\n",
    "            {'role': 'user', 'content': full_prompt},\n",
    "        ])\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def extract_domain_with_regex(text):\n",
    "    \"\"\"Extracts a domain like 'google.com' from a text string using Regex.\"\"\"\n",
    "    # Pattern looks for domain-like structures (word.tld)\n",
    "    pattern = r'\\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,6}\\b'\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    \n",
    "    if matches:\n",
    "        return matches[0] # Return the first valid match\n",
    "    return \"n/a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Clean Data\n",
    "def clean_and_load_data(filepath):\n",
    "    valid_rows = []\n",
    "    missing_data_rows = []\n",
    "    INVALID_VALUES = {'', '-', 'â€”', 'â€“', 'n/a', 'nan', 'none'}\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader, None)\n",
    "            \n",
    "            for row in reader:\n",
    "                if not row: continue\n",
    "                \n",
    "                name, title, company = None, None, None\n",
    "                if len(row) == 4:\n",
    "                    name, title, company = row[0], row[1], row[2]\n",
    "                elif len(row) >= 5:\n",
    "                    name, title, company = row[0], row[1], row[3]\n",
    "                else:\n",
    "                    missing_data_rows.append(row)\n",
    "                    continue\n",
    "\n",
    "                name = name.strip() if name else \"\"\n",
    "                title = title.strip() if title else \"\"\n",
    "                company = company.strip() if company else \"\"\n",
    "\n",
    "                is_title_bad = not title or title.lower() in INVALID_VALUES\n",
    "                is_company_bad = not company or company.lower() in INVALID_VALUES\n",
    "\n",
    "                if is_title_bad or is_company_bad:\n",
    "                    missing_data_rows.append(row)\n",
    "                else:\n",
    "                    valid_rows.append({'Name': name, 'Title': title, 'Company': company})\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{filepath}' not found.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(valid_rows), pd.DataFrame(missing_data_rows)\n",
    "\n",
    "print(\"ðŸ“‚ Loading data...\")\n",
    "df, df_missing = clean_and_load_data(INPUT_FILE)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Process batch trimming\n",
    "if len(df) > BATCH_SIZE:\n",
    "    df = df.head(BATCH_SIZE).copy()\n",
    "    print(f\"âœ‚ï¸  Dataset trimmed to {BATCH_SIZE} rows.\")\n",
    "\n",
    "# Split Names immediately\n",
    "def split_name(full_name):\n",
    "    parts = str(full_name).strip().split()\n",
    "    first = parts[0]\n",
    "    last = \" \".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "    return pd.Series([first, last])\n",
    "\n",
    "if not df.empty:\n",
    "    df[['First_Name', 'Last_Name']] = df['Name'].apply(split_name)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ab9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Senior Executives\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"ðŸ•µï¸  Step 1: Filtering Senior Executives...\")\n",
    "    \n",
    "    def is_senior(title):\n",
    "        prompt = f\"Is '{title}' a Senior Executive role (C-Level, Director, Founder)? Answer strictly YES or NO.\"\n",
    "        res = ask_ollama(prompt)\n",
    "        return \"YES\" in res.upper()\n",
    "\n",
    "    df['Is_Senior'] = df['Title'].apply(is_senior)\n",
    "    \n",
    "    # Filter DataFrame\n",
    "    original_count = len(df)\n",
    "    df = df[df['Is_Senior']].copy()\n",
    "    print(f\"   - Reduced from {original_count} to {len(df)} rows.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Employment & Get Domain\n",
    "# We combine these to loop efficiently\n",
    "\n",
    "def enrich_employment_and_domain(row):\n",
    "    # --- A. Employment Verification ---\n",
    "    # Query: \"Person Company Title 2025 LinkedIn\"\n",
    "    search_query = f\"{row['Name']} {row['Company']} {row['Title']} 2025 LinkedIn\"\n",
    "    work_context = search_web(search_query)\n",
    "    \n",
    "    verify_prompt = f\"Based on the search results, is {row['Name']} still working at {row['Company']} as of 2025? Answer 'Yes', 'Likely No', or 'Uncertain'.\"\n",
    "    still_working = ask_ollama(verify_prompt, work_context)\n",
    "\n",
    "    # --- B. Domain Extraction (With Caching) ---\n",
    "    company_name = row['Company']\n",
    "    \n",
    "    # Check Cache first\n",
    "    if company_name in COMPANY_DOMAIN_CACHE:\n",
    "        domain = COMPANY_DOMAIN_CACHE[company_name]\n",
    "    else:\n",
    "        # Search if not in cache\n",
    "        domain_ctx = search_web(f\"official website domain for {company_name}\")\n",
    "        # Use LLM + Regex to identify domain\n",
    "        raw_domain = ask_ollama(f\"Identify the main official website domain (e.g. apple.com) from these results.\", domain_ctx)\n",
    "        domain = extract_domain_with_regex(raw_domain)\n",
    "        # Store in cache\n",
    "        COMPANY_DOMAIN_CACHE[company_name] = domain\n",
    "\n",
    "    return pd.Series([still_working, domain])\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"ðŸŒ Step 2 & 3: Verifying Employment & Extracting Domains...\")\n",
    "    df[['Employment_Status', 'Domain']] = df.apply(enrich_employment_and_domain, axis=1)\n",
    "    print(\"   - Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Generate Emails\n",
    "# Using the domain and name to guess emails\n",
    "\n",
    "def generate_emails(row):\n",
    "    domain = row['Domain']\n",
    "    if not domain or domain == 'n/a':\n",
    "        return pd.Series([\"N/A\", \"N/A\"])\n",
    "\n",
    "    # 1. Search for pattern\n",
    "    pattern_ctx = search_web(f\"email address format for {row['Company']} {domain} contact\")\n",
    "    \n",
    "    # 2. Ask Ollama to generate\n",
    "    prompt = f\"\"\"\n",
    "    Based on the context (or standard corporate patterns if context is empty), generate the 2 most likely email addresses for:\n",
    "    Name: {row['First_Name']} {row['Last_Name']}\n",
    "    Company Domain: {domain}\n",
    "    \n",
    "    Strict Output Format: email1, email2\n",
    "    (If only one is highly likely, output: email1, email1)\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ask_ollama(prompt, pattern_ctx)\n",
    "    \n",
    "    # Clean up response to get strictly two columns\n",
    "    try:\n",
    "        # Remove any brackets, newlines, or labels\n",
    "        clean_resp = response.replace('\\n', ',').replace('Email 1:', '').replace('Email 2:', '')\n",
    "        emails = [e.strip() for e in clean_resp.split(',') if '@' in e]\n",
    "        \n",
    "        email1 = emails[0] if len(emails) > 0 else \"N/A\"\n",
    "        email2 = emails[1] if len(emails) > 1 else \"N/A\"\n",
    "    except:\n",
    "        email1, email2 = \"Error\", \"Error\"\n",
    "        \n",
    "    return pd.Series([email1, email2])\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"ðŸ“§ Step 4: Generating Emails...\")\n",
    "    df[['Email_1', 'Email_2']] = df.apply(generate_emails, axis=1)\n",
    "    \n",
    "    # Display Result\n",
    "    display(df[['Name', 'Company', 'Employment_Status', 'Domain', 'Email_1', 'Email_2']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f85faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "if not df.empty:\n",
    "    final_cols = ['Name', 'Title', 'Company', 'Domain', 'Employment_Status', 'Email_1', 'Email_2']\n",
    "    # Filter to ensure columns exist\n",
    "    cols_to_save = [c for c in final_cols if c in df.columns]\n",
    "    \n",
    "    df[cols_to_save].to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"ðŸ’¾ Saved {len(df)} validated leads to: {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
