{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration\n",
    "import pandas as pd\n",
    "from ollama import Client\n",
    "from ddgs import DDGS\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "# Use standard tqdm if ipywidgets is still failing, otherwise keep tqdm.notebook\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Register progress_apply with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "MODEL_NAME = \"gemini-3-flash-preview:cloud\"\n",
    "INPUT_FILE = \"overbase_list.csv\"\n",
    "OUTPUT_FILE = \"processed_data.csv\"\n",
    "\n",
    "# --- CONFIGURATION: ROW RANGE ---\n",
    "START_ROW = 0    # Start processing from this row index (Inclusive)\n",
    "END_ROW = 5      # Stop processing before this row index (Exclusive)\n",
    "\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "print(f\"[INFO] System Initialized.\")\n",
    "print(f\"[INFO] AI Model: {MODEL_NAME}\")\n",
    "print(f\"[INFO] Target Range: Rows {START_ROW} to {END_ROW}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions (Search, LLM, Regex)\n",
    "\n",
    "COMPANY_DOMAIN_CACHE = {}\n",
    "\n",
    "def search_web(query, max_results=3):\n",
    "    \"\"\"Searches DuckDuckGo and returns a summary string.\"\"\"\n",
    "    try:\n",
    "        time.sleep(1) # Rate limiting\n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        if not results: return \"\"\n",
    "        return \"\\n\".join([f\"- {r['body']}\" for r in results])\n",
    "    except Exception as e:\n",
    "        # Fail silently to keep the video output clean, or log to file\n",
    "        return \"\"\n",
    "\n",
    "def ask_ollama(prompt, context=\"\"):\n",
    "    \"\"\"Queries the Ollama Cloud model.\"\"\"\n",
    "    full_prompt = f\"\"\"\n",
    "    Context Data:\n",
    "    {context}\n",
    "    \n",
    "    Instruction:\n",
    "    {prompt}\n",
    "    \n",
    "    Output Rules:\n",
    "    - Return ONLY the requested answer.\n",
    "    - No markdown formatting, no intro/outro text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat(model=MODEL_NAME, messages=[\n",
    "            {'role': 'user', 'content': full_prompt},\n",
    "        ])\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def extract_domain_with_regex(text):\n",
    "    \"\"\"Extracts a domain like 'google.com' from a text string using Regex.\"\"\"\n",
    "    pattern = r'\\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,6}\\b'\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    \n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    return \"n/a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfe5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Clean Data\n",
    "def clean_and_load_data(filepath):\n",
    "    valid_rows = []\n",
    "    INVALID_VALUES = {'', '-', '—', '–', 'n/a', 'nan', 'none'}\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader, None)\n",
    "            \n",
    "            for row in reader:\n",
    "                if not row: continue\n",
    "                \n",
    "                # Logic to handle varying CSV column counts\n",
    "                name, title, company = None, None, None\n",
    "                if len(row) == 4:\n",
    "                    name, title, company = row[0], row[1], row[2]\n",
    "                elif len(row) >= 5:\n",
    "                    name, title, company = row[0], row[1], row[3]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                name = name.strip() if name else \"\"\n",
    "                title = title.strip() if title else \"\"\n",
    "                company = company.strip() if company else \"\"\n",
    "\n",
    "                if not title or title.lower() in INVALID_VALUES: continue\n",
    "                if not company or company.lower() in INVALID_VALUES: continue\n",
    "                \n",
    "                valid_rows.append({'Name': name, 'Title': title, 'Company': company})\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] '{filepath}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "print(\"[STEP 1] Loading and Cleaning Data...\")\n",
    "full_df = clean_and_load_data(INPUT_FILE)\n",
    "full_df = full_df.drop_duplicates()\n",
    "\n",
    "# --- RANGE SELECTION LOGIC ---\n",
    "total_available = len(full_df)\n",
    "\n",
    "if START_ROW < total_available:\n",
    "    # Use .iloc to slice by position: [start : end]\n",
    "    df = full_df.iloc[START_ROW:END_ROW].copy()\n",
    "    print(f\"[INFO] Slicing data from row {START_ROW} to {END_ROW}...\")\n",
    "else:\n",
    "    print(f\"[WARN] Start row {START_ROW} is out of bounds (Total rows: {total_available})\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Name Splitting Logic\n",
    "def split_name(full_name):\n",
    "    parts = str(full_name).strip().split()\n",
    "    first = parts[0]\n",
    "    last = \" \".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "    return pd.Series([first, last])\n",
    "\n",
    "if not df.empty:\n",
    "    df[['First_Name', 'Last_Name']] = df['Name'].apply(split_name)\n",
    "    print(f\"[SUCCESS] Slice loaded. Rows to process: {len(df)}\")\n",
    "    print(\"Data Preview (Target Slice):\")\n",
    "    display(df[['Name', 'First_Name', 'Last_Name', 'Title']].head(3))\n",
    "else:\n",
    "    print(\"[ERROR] No data selected. Check your START_ROW/END_ROW config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Senior Executives\n",
    "if not df.empty:\n",
    "    print(\"\\n[STEP 2] Filtering Senior Executives (AI Analysis)...\")\n",
    "    \n",
    "    def is_senior(title):\n",
    "        prompt = f\"Is '{title}' a Senior Executive role (C-Level, Director, VP, Founder)? Answer strictly YES or NO.\"\n",
    "        res = ask_ollama(prompt)\n",
    "        return \"YES\" in res.upper()\n",
    "\n",
    "    # Using progress_apply to show the bar\n",
    "    df['Is_Senior'] = df['Title'].progress_apply(is_senior)\n",
    "    \n",
    "    # Filter DataFrame\n",
    "    original_count = len(df)\n",
    "    df_senior = df[df['Is_Senior']].copy()\n",
    "    \n",
    "    print(f\"[RESULT] Filtering Complete.\")\n",
    "    print(f\"   - Input Rows: {original_count}\")\n",
    "    print(f\"   - Qualified Leads: {len(df_senior)}\")\n",
    "    \n",
    "    display(df_senior[['Name', 'Title', 'Is_Senior']].head())\n",
    "    df = df_senior # Update main dataframe\n",
    "else:\n",
    "    print(\"[WARN] No data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Employment & Get Domain\n",
    "def enrich_employment_and_domain(row):\n",
    "    # --- A. Employment Verification ---\n",
    "    search_query = f\"{row['Name']} {row['Company']} {row['Title']} 2025 LinkedIn\"\n",
    "    work_context = search_web(search_query)\n",
    "    \n",
    "    verify_prompt = f\"Based on the search results, is {row['Name']} still working at {row['Company']} as of 2025? Answer 'Yes', 'Likely No', or 'Uncertain'.\"\n",
    "    still_working = ask_ollama(verify_prompt, work_context)\n",
    "\n",
    "    # --- B. Domain Extraction ---\n",
    "    company_name = row['Company']\n",
    "    \n",
    "    if company_name in COMPANY_DOMAIN_CACHE:\n",
    "        domain = COMPANY_DOMAIN_CACHE[company_name]\n",
    "    else:\n",
    "        domain_ctx = search_web(f\"official website domain for {company_name}\")\n",
    "        raw_domain = ask_ollama(f\"Identify the main official website domain (e.g. apple.com) from these results.\", domain_ctx)\n",
    "        domain = extract_domain_with_regex(raw_domain)\n",
    "        COMPANY_DOMAIN_CACHE[company_name] = domain\n",
    "\n",
    "    return pd.Series([still_working, domain])\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n[STEP 3] Verifying Employment & Extracting Domains...\")\n",
    "    \n",
    "    # Progress bar is crucial here as this is the slowest step\n",
    "    df[['Employment_Status', 'Domain']] = df.progress_apply(enrich_employment_and_domain, axis=1)\n",
    "    \n",
    "    print(\"[RESULT] Enrichment Complete. Updated Data:\")\n",
    "    display(df[['Name', 'Company', 'Employment_Status', 'Domain']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee40025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Emails\n",
    "def generate_emails(row):\n",
    "    domain = row['Domain']\n",
    "    if not domain or domain == 'n/a':\n",
    "        return pd.Series([\"N/A\", \"N/A\"])\n",
    "\n",
    "    # 1. Search for pattern\n",
    "    pattern_ctx = search_web(f\"email address format for {row['Company']} {domain} contact\")\n",
    "    \n",
    "    # 2. Ask Ollama to generate\n",
    "    prompt = f\"\"\"\n",
    "    Based on the context, generate the 2 most likely email addresses for:\n",
    "    Name: {row['First_Name']} {row['Last_Name']}\n",
    "    Company Domain: {domain}\n",
    "    \n",
    "    Strict Output Format: email1, email2\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ask_ollama(prompt, pattern_ctx)\n",
    "    \n",
    "    try:\n",
    "        clean_resp = response.replace('\\n', ',').replace('Email 1:', '').replace('Email 2:', '')\n",
    "        emails = [e.strip() for e in clean_resp.split(',') if '@' in e]\n",
    "        \n",
    "        email1 = emails[0] if len(emails) > 0 else \"N/A\"\n",
    "        email2 = emails[1] if len(emails) > 1 else \"N/A\"\n",
    "    except:\n",
    "        email1, email2 = \"Error\", \"Error\"\n",
    "        \n",
    "    return pd.Series([email1, email2])\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n[STEP 4] Generating Contact Information...\")\n",
    "    df[['Email_1', 'Email_2']] = df.progress_apply(generate_emails, axis=1)\n",
    "    \n",
    "    print(\"[SUCCESS] Pipeline Finalized.\")\n",
    "    display(df[['Name', 'Domain', 'Email_1', 'Email_2']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results (Append Mode)\n",
    "if not df.empty:\n",
    "    final_cols = ['Name', 'Title', 'Company', 'Domain', 'Employment_Status', 'Email_1', 'Email_2']\n",
    "    cols_to_save = [c for c in final_cols if c in df.columns]\n",
    "    \n",
    "    # Check if file exists to determine if we need to write the header\n",
    "    file_exists = os.path.isfile(OUTPUT_FILE)\n",
    "    \n",
    "    # mode='a' appends to the file\n",
    "    # header=not file_exists writes the header only if the file is new\n",
    "    df[cols_to_save].to_csv(OUTPUT_FILE, mode='a', header=not file_exists, index=False)\n",
    "    \n",
    "    print(f\"[SUCCESS] {len(df)} validated leads appended to: {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"[INFO] No data to save for this batch.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
